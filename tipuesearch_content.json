{"pages":[{"title":"About Me","text":"I am currently a data scientist at BlueVine interested in data-science, machine learning, deep learning, Python scientific programming, data visualizations, and Bayesian modelling.","tags":"pages","url":"pages/about-me.html"},{"title":"Apps","text":"See all my apps","tags":"pages","url":"pages/apps.html"},{"title":"My Talks about Disease","text":"During the last few month I was lucky enough to travel around the world and talk about my university work: Disease modeling with Python. Back then, we would have thought that only a few month later this will be all too relevant. @Pydata Warsaw Had a great time talking about how to use python with pymc3 and scipy to model disease behavior and find optimal vaccination strategies. PyData Warsaw is an awesome event at an awesome city. @Olmait Tbilisi I was invited by Olmait to Tbilisi, Georgia to speak in front of the local data-science community about disease modeling, pandas, numpy, pymc3 and much more. Georgia is not (yet) known for its tech scene, but after being exposed to some awesome people, I think it's will be a good bet saying they have a bright future. Webinars Some webinars I gave after the pandemic started","tags":"Talks","url":"talks_disease_modeling.html"},{"title":"My Talk @ PyData Meetup","text":"Diving Into Pandas is Faster than Reinventing it. A talk I gave at the PyData meetup based on the blog post. Some More: The original blog post Tzipi and Sagi's talk: Using Jupyterhub to 10,000x data access Iris' talk: Prevent Incarceration Thru Prioritizing Mental Health Outreach","tags":"Talks","url":"talk_pydata_2018_12.html"},{"title":"Research with Style","text":"Pandas has a great object - the Styler object. It can do wonderful things. Many times, when we research, it's great to visualize the data with with colors. I'm the first one to use Matplotlib when it is needed but sometimes there is just no other way than looking at the data itself. Coloring the data could help a great deal with that. Highlighting null values, understanding the scale of the data or getting a sense of proportions is made a lot easier with styling. In the old days, I used to export my dataframe into an excel file or a google sheet and deploy the reliable conditional formatting. I still love conditional formatting, but exporting tables is something I hate doing, and also Excel and Google Sheets lack the programmatic abilities Python has. So with the styler object you research with style! In [12]: import pandas as pd import blog blog . set_blog_style () Toy Examples with Pandas Testing Pandas testing has a nice method for working with toy examples. More about it here In [14]: import pandas.util.testing as tm tm . N , tm . K = 10 , 7 st = pd . util . testing . makeTimeDataFrame () * 100 st Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E F G 2000-01-03 8.110432 -7.671659 -75.679396 -63.388007 79.634420 10.497984 37.576425 2000-01-04 -18.397552 111.882298 79.171438 53.751517 -107.629410 56.714036 37.317531 2000-01-05 247.767401 -107.709475 -148.387183 58.028000 75.225256 76.857173 72.986559 2000-01-06 -0.674517 42.458056 -33.310198 19.502670 14.905914 -39.948506 -114.569477 2000-01-07 -114.318425 6.137761 -93.587336 -34.042118 -47.988622 -191.936093 2.869611 2000-01-10 164.125587 -53.164714 -117.143462 108.541775 70.644089 133.318074 111.600095 2000-01-11 -29.388656 -242.240230 132.780522 149.456782 70.008844 -88.651034 -156.229086 2000-01-12 110.106442 116.538988 65.999453 38.939958 -6.084684 4.741270 54.417413 2000-01-13 -191.411423 100.878667 -38.618361 9.532799 -229.927711 -2.561390 -13.966168 2000-01-14 -78.295145 -53.193636 63.683134 47.418240 -166.267155 102.587168 -114.575626 And let's insert some null values In [15]: stnan = st . copy () stnan [ np . random . rand ( * stnan . shape ) < 0.05 ] = np . nan The Styler Object Pandas uses an accessor to get a Styler object on the dataframe. This object implements a _repr_html_ which is the method that Jupyter Notebooks use to make the dataframes so nice. You can also export the html. In [16]: tystnan . style # This looks just like the dataframe. Out[16]: A B C D E F G 2000-01-03 00:00:00 8.11043 -7.67166 -75.6794 -63.388 79.6344 10.498 nan 2000-01-04 00:00:00 -18.3976 111.882 79.1714 nan -107.629 56.714 37.3175 2000-01-05 00:00:00 247.767 -107.709 nan 58.028 75.2253 76.8572 72.9866 2000-01-06 00:00:00 -0.674517 42.4581 -33.3102 19.5027 14.9059 -39.9485 -114.569 2000-01-07 00:00:00 -114.318 6.13776 -93.5873 -34.0421 -47.9886 -191.936 2.86961 2000-01-10 00:00:00 164.126 -53.1647 -117.143 108.542 70.6441 nan 111.6 2000-01-11 00:00:00 -29.3887 -242.24 132.781 149.457 70.0088 -88.651 -156.229 2000-01-12 00:00:00 110.106 116.539 65.9995 38.94 -6.08468 4.74127 54.4174 2000-01-13 00:00:00 -191.411 100.879 -38.6184 9.5328 -229.928 -2.56139 -13.9662 2000-01-14 00:00:00 -78.2951 -53.1936 63.6831 47.4182 -166.267 102.587 -114.576 Basic Built-In Styling The styler object has some nice built in functions. You can highlight nulls, min, max, etc. You can also apply it by axis, same as you would on applying functions. In the next example we should expect: nan values to be red each row would have one yellow cell each column would have one blue cell In [17]: ( stnan . style . highlight_null ( 'red' ) . highlight_max ( color = 'steelblue' , axis = 0 ) . highlight_min ( color = 'gold' , axis = 1 ) ) Out[17]: #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow0_col2 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow0_col4 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow0_col6 { background-color: red; : ; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow1_col3 { background-color: red; : ; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow1_col4 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow2_col0 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow2_col1 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow2_col2 { background-color: red; : ; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow3_col6 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow4_col5 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow5_col2 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow5_col5 { background-color: red; : ; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow5_col6 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow6_col1 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow6_col2 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow6_col3 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow7_col1 { : ; background-color: steelblue; : ; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow7_col4 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow8_col4 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow9_col4 { : ; : ; background-color: gold; } #T_3dcde92c_01e3_11e9_af3c_8c859048a2ferow9_col5 { : ; background-color: steelblue; : ; } A B C D E F G 2000-01-03 00:00:00 8.11043 -7.67166 -75.6794 -63.388 79.6344 10.498 nan 2000-01-04 00:00:00 -18.3976 111.882 79.1714 nan -107.629 56.714 37.3175 2000-01-05 00:00:00 247.767 -107.709 nan 58.028 75.2253 76.8572 72.9866 2000-01-06 00:00:00 -0.674517 42.4581 -33.3102 19.5027 14.9059 -39.9485 -114.569 2000-01-07 00:00:00 -114.318 6.13776 -93.5873 -34.0421 -47.9886 -191.936 2.86961 2000-01-10 00:00:00 164.126 -53.1647 -117.143 108.542 70.6441 nan 111.6 2000-01-11 00:00:00 -29.3887 -242.24 132.781 149.457 70.0088 -88.651 -156.229 2000-01-12 00:00:00 110.106 116.539 65.9995 38.94 -6.08468 4.74127 54.4174 2000-01-13 00:00:00 -191.411 100.879 -38.6184 9.5328 -229.928 -2.56139 -13.9662 2000-01-14 00:00:00 -78.2951 -53.1936 63.6831 47.4182 -166.267 102.587 -114.576 Color Scales If you want to understand the scale of the data, applying a gradient creates sort of a \"heat map\" on the table. In the next case - lower values are white while higher values are dark blue. In [18]: st . style . background_gradient () Out[18]: #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col0 { background-color: #86b0d3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col1 { background-color: #2a88bc; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col2 { background-color: #cdd0e5; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col3 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col4 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col5 { background-color: #3790c0; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow0_col6 { background-color: #0f76b3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col0 { background-color: #9fbad9; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col1 { background-color: #023b5d; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col2 { background-color: #05659f; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col3 { background-color: #5c9fc9; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col4 { background-color: #9ebad9; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col5 { background-color: #056dac; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow1_col6 { background-color: #1077b4; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col0 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col1 { background-color: #a7bddb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col2 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col3 { background-color: #509ac6; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col4 { background-color: #023b5d; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col5 { background-color: #04629a; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow2_col6 { background-color: #045d92; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col0 { background-color: #8eb3d5; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col1 { background-color: #0568a3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col2 { background-color: #99b8d8; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col3 { background-color: #a1bbda; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col4 { background-color: #0569a4; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col5 { background-color: #81aed2; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow3_col6 { background-color: #e6e2ef; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col0 { background-color: #e1dfed; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col1 { background-color: #1b7eb7; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col2 { background-color: #dddbec; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col3 { background-color: #e9e5f1; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col4 { background-color: #4897c4; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col5 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow4_col6 { background-color: #4496c3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col0 { background-color: #05659f; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col1 { background-color: #67a4cc; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col2 { background-color: #eee9f3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col3 { background-color: #0566a0; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col4 { background-color: #023f64; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col5 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow5_col6 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col0 { background-color: #a8bedc; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col1 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col2 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col3 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col4 { background-color: #023f64; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col5 { background-color: #b9c6e0; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow6_col6 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col0 { background-color: #1e80b8; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col1 { background-color: #023858; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col2 { background-color: #056dac; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col3 { background-color: #7bacd1; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col4 { background-color: #0f76b3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col5 { background-color: #4094c3; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow7_col6 { background-color: #0569a5; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col0 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col1 { background-color: #03446a; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col2 { background-color: #a1bbda; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col3 { background-color: #b1c2de; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col4 { background-color: #fff7fb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col5 { background-color: #4a98c5; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow8_col6 { background-color: #65a3cb; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col0 { background-color: #ced0e6; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col1 { background-color: #67a4cc; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col2 { background-color: #056fae; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col3 { background-color: #69a5cc; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col4 { background-color: #dad9ea; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col5 { background-color: #045280; } #T_f5ac3d9e_01e5_11e9_824f_8c859048a2ferow9_col6 { background-color: #e6e2ef; } A B C D E F G 2000-01-03 00:00:00 8.11043 -7.67166 -75.6794 -63.388 79.6344 10.498 37.5764 2000-01-04 00:00:00 -18.3976 111.882 79.1714 53.7515 -107.629 56.714 37.3175 2000-01-05 00:00:00 247.767 -107.709 -148.387 58.028 75.2253 76.8572 72.9866 2000-01-06 00:00:00 -0.674517 42.4581 -33.3102 19.5027 14.9059 -39.9485 -114.569 2000-01-07 00:00:00 -114.318 6.13776 -93.5873 -34.0421 -47.9886 -191.936 2.86961 2000-01-10 00:00:00 164.126 -53.1647 -117.143 108.542 70.6441 133.318 111.6 2000-01-11 00:00:00 -29.3887 -242.24 132.781 149.457 70.0088 -88.651 -156.229 2000-01-12 00:00:00 110.106 116.539 65.9995 38.94 -6.08468 4.74127 54.4174 2000-01-13 00:00:00 -191.411 100.879 -38.6184 9.5328 -229.928 -2.56139 -13.9662 2000-01-14 00:00:00 -78.2951 -53.1936 63.6831 47.4182 -166.267 102.587 -114.576 Custom For me this is the best part, with a bit of css you can do anything on your dataframe, This is where we really differ from Excel or Sheets, doing all of this programitacally make life so much easier. In [19]: def custom_style ( val ): if val < - 100 : return 'background-color:red' # Low values are red elif val > 100 : return 'background-color:green' # High values are green elif abs ( val ) < 5 : return 'background-color:yellow' # Values close to 0 are yellow else : return '' st . style . applymap ( custom_style ) Out[19]: #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow1_col1 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow1_col4 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow2_col0 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow2_col1 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow2_col2 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow3_col0 { background-color: yellow; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow3_col6 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow4_col0 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow4_col5 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow4_col6 { background-color: yellow; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow5_col0 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow5_col2 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow5_col3 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow5_col5 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow5_col6 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow6_col1 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow6_col2 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow6_col3 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow6_col6 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow7_col0 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow7_col1 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow7_col5 { background-color: yellow; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow8_col0 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow8_col1 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow8_col4 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow8_col5 { background-color: yellow; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow9_col4 { background-color: red; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow9_col5 { background-color: green; } #T_49851c7e_01e6_11e9_92d3_8c859048a2ferow9_col6 { background-color: red; } A B C D E F G 2000-01-03 00:00:00 8.11043 -7.67166 -75.6794 -63.388 79.6344 10.498 37.5764 2000-01-04 00:00:00 -18.3976 111.882 79.1714 53.7515 -107.629 56.714 37.3175 2000-01-05 00:00:00 247.767 -107.709 -148.387 58.028 75.2253 76.8572 72.9866 2000-01-06 00:00:00 -0.674517 42.4581 -33.3102 19.5027 14.9059 -39.9485 -114.569 2000-01-07 00:00:00 -114.318 6.13776 -93.5873 -34.0421 -47.9886 -191.936 2.86961 2000-01-10 00:00:00 164.126 -53.1647 -117.143 108.542 70.6441 133.318 111.6 2000-01-11 00:00:00 -29.3887 -242.24 132.781 149.457 70.0088 -88.651 -156.229 2000-01-12 00:00:00 110.106 116.539 65.9995 38.94 -6.08468 4.74127 54.4174 2000-01-13 00:00:00 -191.411 100.879 -38.6184 9.5328 -229.928 -2.56139 -13.9662 2000-01-14 00:00:00 -78.2951 -53.1936 63.6831 47.4182 -166.267 102.587 -114.576 Bars Applying bars to your data gives a nice look if you want to understand how your data compares between itself. I know economists have great use for it, but everybody can employ this to get a grasp of the data quickly. In [21]: ( st . style . bar ( subset = [ 'A' , 'D' ], color = 'steelblue' ) . bar ( subset = [ 'G' ], color = [ 'indianred' , 'limegreen' ], align = 'mid' ) ) Out[21]: #T_84085c26_01e6_11e9_bad9_8c859048a2ferow0_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 45.4%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow0_col3 { width: 10em; height: 80%; } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow0_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 72.4%, transparent 72.4%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow1_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 39.4%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow1_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 55.0%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow1_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 72.3%, transparent 72.3%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow2_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 100.0%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow2_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 57.0%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow2_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 85.6%, transparent 85.6%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow3_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 43.4%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow3_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 38.9%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow3_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 15.6%, indianred 15.6%, indianred 58.3%, transparent 58.3%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow4_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 17.6%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow4_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 13.8%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow4_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 59.4%, transparent 59.4%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow5_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 81.0%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow5_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 80.8%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow5_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 100.0%, transparent 100.0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow6_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 36.9%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow6_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 100.0%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow6_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 0.0%, indianred 0.0%, indianred 58.3%, transparent 58.3%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow7_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 68.7%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow7_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 48.1%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow7_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 58.3%, limegreen 58.3%, limegreen 78.6%, transparent 78.6%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow8_col0 { width: 10em; height: 80%; } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow8_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 34.3%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow8_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 53.1%, indianred 53.1%, indianred 58.3%, transparent 58.3%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow9_col0 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 25.8%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow9_col3 { width: 10em; height: 80%; background: linear-gradient(90deg,steelblue 52.1%, transparent 0%); } #T_84085c26_01e6_11e9_bad9_8c859048a2ferow9_col6 { width: 10em; height: 80%; background: linear-gradient(90deg, transparent 0%, transparent 15.6%, indianred 15.6%, indianred 58.3%, transparent 58.3%); } A B C D E F G 2000-01-03 00:00:00 8.11043 -7.67166 -75.6794 -63.388 79.6344 10.498 37.5764 2000-01-04 00:00:00 -18.3976 111.882 79.1714 53.7515 -107.629 56.714 37.3175 2000-01-05 00:00:00 247.767 -107.709 -148.387 58.028 75.2253 76.8572 72.9866 2000-01-06 00:00:00 -0.674517 42.4581 -33.3102 19.5027 14.9059 -39.9485 -114.569 2000-01-07 00:00:00 -114.318 6.13776 -93.5873 -34.0421 -47.9886 -191.936 2.86961 2000-01-10 00:00:00 164.126 -53.1647 -117.143 108.542 70.6441 133.318 111.6 2000-01-11 00:00:00 -29.3887 -242.24 132.781 149.457 70.0088 -88.651 -156.229 2000-01-12 00:00:00 110.106 116.539 65.9995 38.94 -6.08468 4.74127 54.4174 2000-01-13 00:00:00 -191.411 100.879 -38.6184 9.5328 -229.928 -2.56139 -13.9662 2000-01-14 00:00:00 -78.2951 -53.1936 63.6831 47.4182 -166.267 102.587 -114.576","tags":"Talks","url":"research_with_style.html"},{"title":"My New Website: epidemic.co.il","text":"My new website Epidemic (Hebrew) is up and running. Epidemic is an open data public health project focused on making disease data accessible to the public. The website uses mainly Bokeh with some basic css and javascript, and has many options such as: Viewing disease progression chart for dozens of disease Viewing disease progression by regions in the country Downloading data as machine-readable CSVs Viewing disease information Dynamic ranges Bar chart to easily understand aggregated cases by region I am really proud of this project as the Israeli Ministry of Health serves this data in a non friendly website with many messy Excel files. The first part of this project was to crawl the website and get the data. This project is in my github and is free for use. The second part is the Bokeh server on top of this crawler, which has taught my a lot. I will hopefully show some the things I've learned in future posts.","tags":"Talks","url":"introducing-epidemic.html"},{"title":"Diving into Pandas is Faster than Reinventing it","text":"The following post was first presented as a talk for the IE@DS community. It will also be presented at PyData meetup in December. All the resources for this post, including a runable notebook, can be found in the github repo . Let's begin This notebook aims to show some nice ways modern Pandas makes your life easier. It is not about efficiency. I'm pretty sure using Pandas' built-in methods will be more efficient than reinventing pandas, but the main goal is to make the code easier to read, and more imoprtant - easier to write. In [6]: import sys import os sys . path . append ( os . path . dirname ( os . getcwd ())) In [7]: import pandas as pd import numpy as np import matplotlib.pyplot as plt import os import sys import blog % matplotlib inline import my_utils blog . set_blog_style () First Hacks! Reading the data and a few housekeeping tasks. This is the first place we can make our code more readable. In [8]: df_io = pd . read_csv ( './data.csv' , index_col = 0 , parse_dates = [ 'date_' ]) df_io . head () Out[8]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } val_updated date_ event_type 0 2018-10-09 17:01:30.662390+00:00 2018-10-09 17:01:30.662389 5 1 2018-10-09 17:01:30.687675+00:00 2018-10-09 17:01:30.687675 5 2 2018-10-09 17:03:23.952848+00:00 2018-10-09 17:03:23.952847 1 3 2018-10-09 17:05:42.327744+00:00 2018-10-09 17:05:42.327744 7 4 2018-09-24 16:36:38.177661+00:00 2018-10-09 17:06:29.708909 7 In [9]: df = df_io . copy () . sort_values ( 'date_' ) . set_index ( 'date_' ) . drop ( columns = 'val_updated' ) df . head () Out[9]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:01:30.662389 5 2018-10-09 17:01:30.687675 5 2018-10-09 17:03:23.952847 1 2018-10-09 17:05:42.327744 7 2018-10-09 17:06:29.708909 7 Beautiful pipes! One line method chaining is hard to read and prone to human error, chaining each method in its own line makes it a lot more readable. In [10]: df_io \\ . copy () \\ . sort_values ( 'date_' ) \\ . set_index ( 'date_' ) \\ . drop ( columns = 'val_updated' ) \\ . head () Out[10]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:01:30.662389 5 2018-10-09 17:01:30.687675 5 2018-10-09 17:03:23.952847 1 2018-10-09 17:05:42.327744 7 2018-10-09 17:06:29.708909 7 But it has a problem. You can't comment out and even comment in between In [11]: # This block will result in an error df_io \\ . copy () \\ # This is an inline comment # This is a regular comment . sort_values ( 'date_' ) \\ # .set_index('date_')\\ . drop ( columns = 'val_updated' ) \\ . head () File \"<ipython-input-11-f6a35867b44d>\" , line 2 df_io.copy()\\ # This is an inline comment &#94; SyntaxError : unexpected character after line continuation character Even an unnoticeable space character may break everything In [12]: # This block will result in an error df_io \\ . copy () \\ . sort_values ( 'date_' ) \\ . set_index ( 'date_' ) \\ . drop ( columns = 'val_updated' ) \\ . head () File \"<ipython-input-12-708b932ea07a>\" , line 2 df_io.copy().sort_values('date_').set_index('date_').drop(columns='val_updated')\\ &#94; SyntaxError : unexpected character after line continuation character The Penny Drops I like those \"penny dropping\" moments, when you realize you knew everything that is presented, yet it is presented in a new way you never thought of. In [13]: # We can split these value inside () users = ( 134856 , 195373 , 295817 , 294003 , 262166 , 121066 , 129678 , 307120 , 258759 , 277922 , 220794 , 192312 , 318486 , 314631 , 306448 , 297059 , 206892 , 169046 , 181703 , 146200 , 199876 , 247904 , 250884 , 282989 , 234280 , 202520 , 138064 , 133577 , 301053 , 242157 ) In [14]: # Penny Drop: We can also Split here df = ( df_io . copy () # This is an inline comment # This is a regular comment . sort_values ( 'date_' ) . set_index ( 'date_' ) . drop ( columns = 'val_updated' ) ) df . head () Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:01:30.662389 5 2018-10-09 17:01:30.687675 5 2018-10-09 17:03:23.952847 1 2018-10-09 17:05:42.327744 7 2018-10-09 17:06:29.708909 7 Map with dict A dict is a callable with $f(key) = value$, there for you can call .map with it. In this example I want to make int key codes into letter. In [15]: df . event_type . map ( lambda x : x + 3 ) . head () Out[15]: date_ 2018-10-09 17:01:30.662389 8 2018-10-09 17:01:30.687675 8 2018-10-09 17:03:23.952847 4 2018-10-09 17:05:42.327744 10 2018-10-09 17:06:29.708909 10 Name: event_type, dtype: int64 In [16]: # A dict is also a calleble df [ 'event_type' ] = df . event_type . map ({ 1 : 'A' , 5 : 'B' , 7 : 'C' }) df . head () Out[16]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:01:30.662389 B 2018-10-09 17:01:30.687675 B 2018-10-09 17:03:23.952847 A 2018-10-09 17:05:42.327744 C 2018-10-09 17:06:29.708909 C Time Series Resample Task: How many events happen each hour? The Old Way In [17]: bad = df . copy () bad [ 'day' ] = bad . index . date bad [ 'hour' ] = bad . index . hour ( bad . groupby ([ 'day' , 'hour' ]) . count () ) Out[17]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type day hour 2018-10-09 17 83 18 96 20 91 21 71 22 84 Many lines of code unneeded columns Index is not a time anymore missing rows (Did you notice?) A Better Way In [18]: df . resample ( 'H' ) . count () # H is for Hour Out[18]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:00:00 83 2018-10-09 18:00:00 96 2018-10-09 19:00:00 0 2018-10-09 20:00:00 91 2018-10-09 21:00:00 71 2018-10-09 22:00:00 84 But it's even better on non-round intervals In [19]: rs = df . resample ( '10T' ) . count () # T is for Minute, and pandas understands 10 T, it will also under stand 11T if you wonder rs . head () Out[19]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:00:00 7 2018-10-09 17:10:00 12 2018-10-09 17:20:00 16 2018-10-09 17:30:00 13 2018-10-09 17:40:00 15 Complete list of Pandas' time abbrevations Slice Easily Pandas will automatically make string into timestamps, and it will understand what you want it to do. In [20]: # Take only timestamp in the hour of 21:00. rs . loc [ '2018-10-09 21' ,:] Out[20]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 21:00:00 11 2018-10-09 21:10:00 18 2018-10-09 21:20:00 16 2018-10-09 21:30:00 9 2018-10-09 21:40:00 7 2018-10-09 21:50:00 10 In [21]: # Take all time stamps berfore 18:31 rs . loc [: '2018-10-09 18:31' ,:] Out[21]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type date_ 2018-10-09 17:00:00 7 2018-10-09 17:10:00 12 2018-10-09 17:20:00 16 2018-10-09 17:30:00 13 2018-10-09 17:40:00 15 2018-10-09 17:50:00 20 2018-10-09 18:00:00 21 2018-10-09 18:10:00 22 2018-10-09 18:20:00 4 2018-10-09 18:30:00 19 Time Windows: Rolling, Expanding, EWM If your Dataframe is indexed on a datetime index (Which ours is), you have many options for window functions, which are again sort of \"groupby\" operations that in old times we used to do on our own. In [22]: fig , ax = plt . subplots () rs . plot ( ax = ax , linestyle = '--' ) ( rs . rolling ( 6 ) . mean () . rename ( columns = { 'event_type' : 'rolling mean' }) . plot ( ax = ax ) ) rs . expanding ( 6 ) . mean () . rename ( columns = { 'event_type' : 'expanding mean' }) . plot ( ax = ax ) rs . ewm ( 6 ) . mean () . rename ( columns = { 'event_type' : 'ewm mean' }) . plot ( ax = ax ) plt . show () With Apply Intuitively, windows are like GroupBy, so you can apply anything you want after the grouping, e.g.: geometric mean. In [25]: fig , ax = plt . subplots () rs . plot ( ax = ax , linestyle = '--' ) ( rs . rolling ( 6 ) . apply ( lambda x : np . power ( np . product ( x ), 1 / len ( x )), raw = True ) . rename ( columns = { 'event_type' : 'Rolling Geometric Mean' }) . plot ( ax = ax ) ) plt . show () Combine with GroupBy ðŸ¤¯ Pandas has no problem with groupby and resample together. It's as simple as groupby[col1,col2] . In our specific case, we want to cound events in an interval per event type. In [26]: per_event = ( df . groupby ( 'event_type' ) . resample ( '15T' ) . apply ( 'count' ) . rename ( columns = { 'event_type' : 'amount' }) ) per_event . head () Out[26]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ A 2018-10-09 17:00:00 8 2018-10-09 17:15:00 6 2018-10-09 17:30:00 6 2018-10-09 17:45:00 9 2018-10-09 18:00:00 3 Stack, Unstack Unstack In this case, working with a wide format indexed on intervals, with event types as columns, will make a lot more sense. The Old way Pivot table in modern pandas is more robust than it used to be. Still, it requires you to specify everything. In [27]: pt = pd . pivot_table ( per_event , values = 'amount' , columns = 'event_type' , index = 'date_' ) pt . head () Out[27]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C date_ 2018-10-09 17:00:00 8 4 2 2018-10-09 17:15:00 6 11 4 2018-10-09 17:30:00 6 9 4 2018-10-09 17:45:00 9 10 10 2018-10-09 18:00:00 3 18 7 A better way When you have just one column of values, unstack does the same easily In [28]: pt = per_event . unstack ( 'event_type' ) pt . columns = pt . columns . droplevel () # Unstack creates a multiindex on columns pt . head () Out[28]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C date_ 2018-10-09 17:00:00 8 4 2 2018-10-09 17:15:00 6 11 4 2018-10-09 17:30:00 6 9 4 2018-10-09 17:45:00 9 10 10 2018-10-09 18:00:00 3 18 7 Unstack And some extra tricks In [29]: pt . stack () . head () Out[29]: date_ event_type 2018-10-09 17:00:00 A 8 B 4 C 2 2018-10-09 17:15:00 A 6 B 11 dtype: int64 This looks kind of what we had expected but: It's a series, not a DataFrame The levels of the index are reversed The main sort is on the date, yet it used to be on the event type In [30]: stack_back = ( pt . stack () . to_frame ( 'amount' ) # Turn Series to DF without calling the DF constructor . swaplevel () # Swaps the levels of the index . sort_index () # Sort by index, makes so much sense yet I used to do .reset_index().sort_values() ) stack_back . head () Out[30]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ A 2018-10-09 17:00:00 8 2018-10-09 17:15:00 6 2018-10-09 17:30:00 6 2018-10-09 17:45:00 9 2018-10-09 18:00:00 3 In [31]: stack_back . equals ( per_event ) Out[31]: True Sorting By Values In [38]: per_event . sort_values ( by = [ 'amount' ]) . head ( 10 ) Out[38]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ B 2018-10-09 19:45:00 0 2018-10-09 19:00:00 0 2018-10-09 19:15:00 0 2018-10-09 19:30:00 0 C 2018-10-09 19:00:00 0 A 2018-10-09 19:30:00 0 2018-10-09 19:15:00 0 2018-10-09 19:00:00 0 2018-10-09 19:45:00 0 C 2018-10-09 19:30:00 0 By Index In [39]: per_event . sort_index () . head ( 7 ) Out[39]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ A 2018-10-09 17:00:00 8 2018-10-09 17:15:00 6 2018-10-09 17:30:00 6 2018-10-09 17:45:00 9 2018-10-09 18:00:00 3 2018-10-09 18:15:00 2 2018-10-09 18:30:00 1 In [40]: per_event . sort_index ( level = 1 ) . head ( 7 ) Out[40]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ A 2018-10-09 17:00:00 8 B 2018-10-09 17:00:00 4 C 2018-10-09 17:00:00 2 A 2018-10-09 17:15:00 6 B 2018-10-09 17:15:00 11 C 2018-10-09 17:15:00 4 A 2018-10-09 17:30:00 6 By Both (New in 0.23) If the index has a name, just treat it has a column. In [41]: per_event . sort_values ([ 'amount' , 'event_type' ], ascending = ( False , True )) . head ( 10 ) Out[41]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } amount event_type date_ B 2018-10-09 18:00:00 18 2018-10-09 20:30:00 17 2018-10-09 18:30:00 16 2018-10-09 22:45:00 14 A 2018-10-09 18:45:00 12 2018-10-09 20:45:00 12 B 2018-10-09 22:00:00 12 2018-10-09 22:30:00 12 C 2018-10-09 20:30:00 12 A 2018-10-09 22:15:00 11 Clip Let's say, we know from domain knowledge the that an event takes place a minimum of 5 and maximum of 12 at each timestamp. We would like to fix that. In a real world example, we many time want to turn negative numbers to zeroes or some truly big numbers to sum known max. The Old Way Iterate over columns and change values that meet condition. In [32]: cl = pt . copy () lb = 5 ub = 12 # Needed A loop of 3 lines for col in [ 'A' , 'B' , 'C' ]: cl [ 'clipped_ {} ' . format ( col )] = cl [ col ] cl . loc [ cl [ col ] < lb , 'clipped_ {} ' . format ( col )] = lb cl . loc [ cl [ col ] > ub , 'clipped_ {} ' . format ( col )] = ub my_utils . plot_clipped ( cl ) # my_utils can be found in the github repo A better way .clip(lb,ub) In [33]: cl = pt . copy () # Beutiful One Liner cl [[ 'clipped_A' , 'clipped_B' , 'clipped_C' ]] = cl . clip ( 5 , 12 ) my_utils . plot_clipped ( cl ) # my_utils can be found in the github repo Reindex Now I have 3 event types from 17:00 to 23:00. Let's imagine, I know that actually I have 5 event types. I also know that the period was from 16:00 to 00:00. In [34]: etypes = list ( 'ABCDZ' ) # New columns # Define a date range - Pandas will automatically make this into an index idx = pd . date_range ( start = '2018-10-09 16:00:00' , end = '2018-10-09 23:59:00' , freq = '15T' ) type ( idx ) Out[34]: pandas.core.indexes.datetimes.DatetimeIndex In [35]: pt . reindex ( idx , columns = etypes , fill_value = 0 ) . head () Out[35]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C D Z 2018-10-09 16:00:00 0 0 0 0 0 2018-10-09 16:15:00 0 0 0 0 0 2018-10-09 16:30:00 0 0 0 0 0 2018-10-09 16:45:00 0 0 0 0 0 2018-10-09 17:00:00 8 4 2 0 0 In [36]: ### Let's put this in a function - This will help us later. def get_all_types_and_timestamps ( df , min_date = '2018-10-09 16:00:00' , max_date = '2018-10-09 23:59:00' , etypes = list ( 'ABCDZ' )): ret = df . copy () time_idx = pd . date_range ( start = min_date , end = max_date , freq = '15T' ) # Indices work like set. This is a good practive so we don't override our intended index idx = ret . index . union ( time_idx ) etypes = df . columns . union ( set ( etypes )) ret = ret . reindex ( idx , columns = etypes , fill_value = 0 ) return ret Method Chaining Assign Assign is for creating new columns on the dataframes. This is instead of df[new_col] = function(df[old_col]) . They are both one lines, but .assign doesn't break the flow. In [37]: pt . assign ( mean_all = pt . mean ( axis = 1 )) . head () Out[37]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C mean_all date_ 2018-10-09 17:00:00 8 4 2 4.666667 2018-10-09 17:15:00 6 11 4 7.000000 2018-10-09 17:30:00 6 9 4 6.333333 2018-10-09 17:45:00 9 10 10 9.666667 2018-10-09 18:00:00 3 18 7 9.333333 Pipe Think R's %>% (Or rather, avoid thinking about R), .pipe is a method that accepts a function. pipe , by default, assumes the first argument of this function is a dataframe and passes the current dataframe down the pipeline. The function should return a dataframe also, if you want to continue with the pipe. Yet, it can also return any other value if you put it in the last step. This is incredibly valueable because it takes you one step further from \"sql\" where you do things \"in reverse\". $f(g(h(df)))$ = df.pipe(h).pipe(g).pipe(f) In [46]: def do_something ( df , col = 'A' , n = 200 ): ret = df . copy () # A dataframe is mutable, if you don't copy it first, this is prone to many errors. # I always copy when I enter a function, even if I'm sure it shouldn't change anything. ret [ col ] = ret [ col ] + n return ret In [47]: do_something ( do_something ( do_something ( pt ), 'B' , 100 ), 'C' , 500 ) . head () Out[47]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C date_ 2018-10-09 17:00:00 208 104 502 2018-10-09 17:15:00 206 111 504 2018-10-09 17:30:00 206 109 504 2018-10-09 17:45:00 209 110 510 2018-10-09 18:00:00 203 118 507 In [48]: ( pt . pipe ( do_something ) . pipe ( do_something , col = 'B' , n = 100 ) . pipe ( do_something , col = 'C' , n = 500 ) . head ( 5 )) Out[48]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } event_type A B C date_ 2018-10-09 17:00:00 208 104 502 2018-10-09 17:15:00 206 111 504 2018-10-09 17:30:00 206 109 504 2018-10-09 17:45:00 209 110 510 2018-10-09 18:00:00 203 118 507 You can always do this with multiple lines of df = do_something(df) but I think this method is more elegant. Beautiful Code Tells a Story Your code is not just about making the computer do things. It's about telling a story of what you wish to happen. Sometimes other people will want to read you code. Most time, it is you 3 months in the future who will read it. Some say good code documents itself; I'm not that extreme. Yet, storytelling with code may save you from many lines of unnecessary comments. The next and final block tells the story in one block, or rather one pipe. It's elegant, it tells a story. If you build utility functions and pipe them while following meaningful naming, they help tell a story. if you assign columns with meaningful names, they tell a story. you drop , you apply , you read , you groupby and you resample - they all tell a story. (Well... Maybe they could have gone with better naming for resample ) In [52]: df = ( pd . read_csv ( './data.csv' , index_col = 0 , parse_dates = [ 'date_' ]) . assign ( event_type = lambda df : df . event_type . map ({ 1 : 'A' , 5 : 'B' , 7 : 'C' })) . sort_values ( 'date_' ) . set_index ( 'date_' ) . drop ( columns = 'val_updated' ) . groupby ( 'event_type' ) . resample ( '15T' ) . apply ( 'count' ) . rename ( columns = { 'event_type' : 'amount' }) . unstack ( 'event_type' ) . pipe ( my_utils . remove_multi_index ) . pipe ( get_all_types_and_timestamps ) # Remember this from before? . assign ( mean_event = lambda df : df . mean ( axis = 1 )) . loc [:, [ 'mean_event' ]] . pipe ( my_utils . make_sliding_time_windows , steps_back = 6 ) # Imagine last pipe is an api. Not sure how it's implemented, but the name is meaningful . dropna () ) In [54]: df . head () Out[54]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_event mean_event_1 mean_event_2 mean_event_3 mean_event_4 mean_event_5 mean_event_6 2018-10-09 17:30:00 3.8 4.2 2.8 0.0 0.0 0.0 0.0 2018-10-09 17:45:00 5.8 3.8 4.2 2.8 0.0 0.0 0.0 2018-10-09 18:00:00 5.6 5.8 3.8 4.2 2.8 0.0 0.0 2018-10-09 18:15:00 3.8 5.6 5.8 3.8 4.2 2.8 0.0 2018-10-09 18:30:00 5.0 3.8 5.6 5.8 3.8 4.2 2.8 You don't have to memorize this Just put this in the back of your mind and remember that modern Pandas has so many superpowers. Just remember they exist, and google them when you actually need them. Always, when I feel I'm insecure about Pandas, I go back to Greg Reda 's tweet : Resources Modern Pandas by Tom Augspurger Basic Time Series Manipulation with Pandas by Laura Fedoruk Pandas Docs . You don't have to thoroughly go over everything, just randomly open a page in the docs and you're sure to learn a new thing.","tags":"Tutorial","url":"dont_reinvent_pandas.html"},{"title":"My Talk @ PyCon IL","text":"","tags":"Talks","url":"talk_pycon_2018.html"},{"title":"My Talk @ DS Summit","text":"","tags":"Talks","url":"talk_ds_summit_2018.html"},{"title":"Spacious Matplotlib Ticks","text":"A recent issue I had in my research is tick labels that overlap each other. It happens when there are too many leading zeros before the significant digit, yet too loo little for matplotlib to intelligently shift to scientific notation (about 4-7 leading zeroes). That result in the tick labels overlapping and making everything unreadable. It is not that much of a problem on exploratory code, but when I need a paper-worthy chart, it's an issue. In [1]: import numpy as np # import pandas as pd import matplotlib.pyplot as plt % matplotlib inline In [13]: # Create random data of x = np . random . normal ( loc = 0.0000035 , scale = 0.000002 , size = 5000 ) print ( x ) [ 3.84484418e-06 2.91685770e-06 3.88867504e-06 ..., 4.26532717e-06 3.18812399e-06 4.30168460e-06] I have 5000 samples from the normal distribution. The samples have about 6 leading zeroes. Next, I'll create a histogram for the samples. In [14]: fig , axs = plt . subplots ( figsize = ( 6 , 4 )) axs . hist ( x ) plt . show () You can see how the labels of the x axis are overlapping and you can't understand any of them. I wanted a fast yet elegant way to deal with this. I didn't want to mess with rcParams for now (Although probably better in the long run). .get_xticks() method returns a numpy array of all the labels. Now with the easy slicing mechanism of numpy arrays I can get all even places. .set_xticks() method can set the new ticks to that value. In [15]: fig , axs = plt . subplots ( figsize = ( 6 , 4 )) axs . hist ( x ) axs . set_xticks ( axs . get_xticks ()[:: 2 ]) plt . show () There are better practices if you have many ticks on a many axes object, but this is a nice, elegant hack if you need it for a few paper-worthy charts, without the hassle.","tags":"Hacks","url":"spacious-matplotlib-tickss.html"}]}